import numpy as np
import pandas as pd
import gym
import os, pdb, random
from tqdm import tqdm

from gym import spaces
from gym.utils import seeding

class StockMarketEnv(gym.Env):
    """2D navigation problems, as described in [1]. The code is adapted from 
    https://github.com/cbfinn/maml_rl/blob/9c8e2ebd741cb0c7b8bf2d040c4caeeb8e06cc95/maml_examples/point_env_randgoal.py

    At each time step, the 2D agent takes an action (its velocity, clipped in
    [-0.1, 0.1]), and receives a penalty equal to its L2 distance to the goal 
    position (ie. the reward is `-distance`). The 2D navigation tasks are 
    generated by sampling goal positions from the uniform distribution 
    on [-0.5, 0.5]^2.

    [1] Chelsea Finn, Pieter Abbeel, Sergey Levine, "Model-Agnostic 
        Meta-Learning for Fast Adaptation of Deep Networks", 2017 
        (https://arxiv.org/abs/1703.03400)
    """
    def __init__(self, task={}, lookback=30):
        super(StockMarketEnv, self).__init__()
        self.lookback = lookback
        self.read()

        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(lookback,), dtype=np.float32)
        self.action_space = spaces.Discrete(3)

        if len(task) != 0 :
            self._task = task
            self._episode = task.get('episode')
            self._state = self._episode[:lookback]
        self.seed()

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def read(self,):
        self.data = []
        datadir = 'D:/data_repository/S&P500_price_data/'
        print('reading data...')
        for stockfile in tqdm(os.listdir(datadir)[:20]) :
            df = pd.read_csv(datadir+stockfile)
            arr = np.array(df[df.Date>'2012']['Adj. Close'])
            ret = 100*(arr[1:]/arr[:-1]-1)
            if arr.shape[0] > 1200 :
                self.data.append(ret)
        # pdb.set_trace()

    def sample_tasks(self, num_tasks):
        samples = random.sample(self.data, num_tasks)
        idxs = [random.sample(range(smpl.shape[0]-252),1)[0] for smpl in samples]
        tasks = [{'episode':smpl[idx:idx+252]} for idx, smpl in zip(idxs, samples)]
        return tasks

    def reset_task(self, task):
        self._task = task
        self._episode = task['episode']

    def reset(self, env=True):
        self._state = self._episode[:self.lookback]
        return self._state

    def step(self, action):
        action = np.clip(action, -0.1, 0.1)
        assert self.action_space.contains(action)
        self._state = self._state + action

        x = self._state[0] - self._goal[0]
        y = self._state[1] - self._goal[1]
        reward = -np.sqrt(x ** 2 + y ** 2)
        if reward <= -0.03 :
            reward = -100.0

        done = ((np.abs(x) < 0.01) and (np.abs(y) < 0.01))

        return self._state, reward, done, {'task': self._task}
